import pyspark
import random

# Connect to the spark cluster
import pyspark
conf=pyspark.SparkConf().setMaster('spark://spy3:7077') \
     .set('spark.driver.host', 'jupyter-notebook-py36') \
     .set('spark.driver.port', 42000) \
     .set('spark.driver.bindAddress', '0.0.0.0') \
     .set('spark.driver.blockManager.port', 42100)
sc=pyspark.SparkContext(conf=conf)`

# Run the pi calculation
num_samples = 100000000
def inside(p):     
  x, y = random.random(), random.random()
  return x*x + y*y < 1
count = sc.parallelize(range(0, num_samples)).filter(inside).count()
pi = 4 * count / num_samples
print(pi)

# Stop the spark context
sc.stop()